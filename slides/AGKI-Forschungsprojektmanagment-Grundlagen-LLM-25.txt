Slides were generated AI-assisted. Images are partly AI-generated.
Large Language Models & Prompt Engineering Grundlagen

Angewandte Generative KI und Prompt Engineering im Forschungsprojektmanagement (Projektmanagement)
Dr. Christopher Pollin
https://chpollin.github.io | christopher.pollin@dhcraft.org 
Digital Humanities Craft OGwww.dhcraft.org 

LLMs führen Next Token Prediction durch. Sie sagen das nächste Token in einer Folge von Tokens (~ Kontext) auf Grundlage ihrer Trainingsdaten voraus. Jedes vorhergesagte Token wird Teil des Kontexts für die nächste Vorhersage (autoregressiv). Dieser einfache Mechanismus, massiv skaliert, erzeugt die “schwach emergenten” (?) Eigenschaften, die wir beobachten.
Wie LLMs funktionieren
Ethan Mollick. Thinking Like an AI. https://www.oneusefulthing.org/p/thinking-like-an-ai  
Andrej Karpathy. Deep Dive into LLMs like ChatGPT. https://youtu.be/7xTGNNLPyMI Andrej Karpathy. How I use LLMs. https://youtu.be/EWvNQjAaOHw Andrej Karpathy. [1hr Talk] Intro to Large Language Models. https://www.youtube.com/watch?v=zjkBMFhNj_g Alan Smith. Inside GPT – Large Language Models Demystified https://youtu.be/MznD2DzlQCc3Blue1Brown. But what is a GPT? Visual intro to transformers | Chapter 5, Deep Learning. https://youtu.be/wjZofJX0v4M  Ethan Mollick. Thinking Like an AI. A little intuition can help. https://www.oneusefulthing.org/p/thinking-like-an-ai    

Andrej Karpathy. How I use LLMs. https://youtu.be/EWvNQjAaOHw
Andrej Karpathy. [1hr Talk] Intro to Large Language Models. https://www.youtube.com/zjkBMFhNj_g
Pre-Training (“Kompression von Wissen”)

Input: Trillionen von Tokens aus (Web-)Daten 
und/oder synthetischen Daten
Task: das nächste Token vorhersagen
Eigenschaften:
verlustbehaftet (nicht perfekter Speicher)
probabilistisch (Muster, keine Fakten)
Wissensabgrenzung (zeitlich festgelegt)
Kosten Sehr teuer (Geld, Energie, GPU), langsam


“Große Sprachmodelle sind verlustbehaftete, probabilistische Komprimierungen („.zip“) von möglichst vielen hochwertigen (multimodalen) Daten.”

Die Gestalt eines Wikipedia-Artikels über Zebras
LLMs können nicht direkt auf Wikipedia-Artikel zugreifen. Sie haben nur Zugriff auf die Gestalt (Karpathy) des Textes, die komprimierte statistische Muster darstellt, die während des Trainings entstehen.

LLMs besuchen Webseiten nicht! Sie können jedoch Tools für die Websuche verwenden (Tool Use).

Interne Wissensrepräsentation des Modells im Vergleich zu seiner Fähigkeit, über Tools auf externe Informationen zuzugreifen
https://de.wikipedia.org/wiki/Zebras 

Character Claude
https://www.anthropic.com/research/claude-character 

https://www.youtube.com/watch?v=ugvHCXCOmm4&t=9774s 

Energie: 0,0003 kWh pro Prompt (= 8–10 Sekunden Netflix-Streaming)
Wasser: 0.25-5mL pro Prompt (feinige Tropfen bis zu 1/5 eines Schnapsglases)
Effizienz: 33-fache Verbesserung in einem Jahr (Google)
Kosten: $50→$0.14 per million tokens (GPT-4 to GPT-5 nano)
Die USA investieren Hunderte von Milliarden 
in Rechenzentren und Energieerzeugung.
Meta Builds Manhattan-Sized AI Data Centers in Multi-Billion Dollar Tech Race. https://www.ctol.digital/news/meta-builds-manhattan-sized-ai-data-centers-tech-race/ 
Inside OpenAI's Stargate Megafactory with Sam Altman | The Circuit. https://youtu.be/GhIJs4zbH0o
Ethan Mollick. Mass Intelligence. From GPT-5 to nano banana: everyone is getting access to powerful AI https://www.oneusefulthing.org/p/mass-intelligence 
Jegham, Nidhal, Marwen Abdelatti, Lassad Elmoubarki, and Abdeltawab Hendawi. ‘How Hungry Is AI? Benchmarking Energy, Water, and Carbon Footprint of LLM Inference’. 14 May 2025. https://doi.org/10.48550/arXiv.2505.09598. 
Während einzelne LLM-Abfragen immer effizienter werden, führt ihr massiver Einsatz zu einem Paradoxon, bei dem allein GPT-4o jährlich so viel Strom verbraucht wie 35.000 US-Haushalte. Dies zeigt, dass die Wahl der Infrastruktur für die Umweltbelastung wichtiger ist als die Modellgröße und dass die weltweite Einführung von KI einen Ressourcenverbrauch verursacht, der die Effizienzgewinne bei weitem übersteigt.

Deep Dive into LLMs like ChatGPT. https://youtu.be/7xTGNNLPyMI
Let's build the GPT Tokenizer. https://youtu.be/zduSFxRajkE
Tokenization
Rohtext aus dem Internet:
“Hello World!”
Cleaning und Filtering 
(removes spam, deduplication)

Tokenizer
[‘Hello’, ‘World’, ‘!’]
IDs 
[13225, 5922, 0]

Die Tokenisierung wandelt Text in numerische Einheiten für die LLM-Verarbeitung um. Die Tokenisierungsstrategie priorisiert die Recheneffizienz, indem sie die Sequenzlänge minimiert.

Ein Token ist die atomare Einheit für LLMs 
(100 Token ≈ 75 englische Wörter).
https://huggingface.co/datasets/HuggingFaceFW/fineweb 
https://platform.openai.com/tokenizer
https://tiktokenizer.vercel.app/ 

Warum kann ein LLM nicht buchstabieren und warum sieht man so viele „:” und „ー”? 
Let’s talk about em dashes in AI. Maria Sukhavera. https://msukhareva.substack.com/p/lets-talk-about-em-dashes-in-ai 
AI Slop

9
Transformer-Architecture
https://jalammar.github.io/illustrated-transformer
3Blue1Brown. But what is a GPT? Visual intro to transformers | Chapter 5, Deep Learning
Andrej Karpathy. [1hr Talk] Intro to Large Language Models. https://www.youtube.com/watch?v=zjkBMFhNj_g
Alan Smith. Inside GPT – Large Language Models Demystified, 2024
https://youtu.be/MznD2DzlQCc

Model Context Window = 8K
A context window, in the context of large language models (LLMs), refers to the portion of text that the model can consider at once when generating or analyzing language.[...]
Model Context Window = 8K
A context window, in the context of large language models (LLMs), refers to the portion of text that the model can consider at once when generating or analyzing language. It is essentially the window through which the model "sees" and processes text, helping it understand the current context to make predictions, generate coherent sentences, or provide relevant responses.[...]
6000 Token
10000 Token
Input Token
Output Token
Lorem ipsum … 
Lorem ipsum … 
1500 Token
1500 Token
Context Window = 6000 + 1500 < 8000
Context Window = 10000 + 1500 > 80003500 tokens are not in the context window!
What is a Context Window? Unlocking LLM Secrets. https://youtu.be/-QVoIxEpFkM

Dingemanse, Mark. 2024. „Generative AI and Research Integrity“. Preprint, OSF, Mai 14. https://doi.org/10.31219/osf.io/2c48n.
 

Deep Dive into LLMs like ChatGPT. https://youtu.be/7xTGNNLPyMI
Let's build the GPT Tokenizer. https://youtu.be/zduSFxRajkE
Embeddings

Ähnliche Bedeutungen = Nähere Positionen im Raum
“dog” und “cat” → nah 
(beides Haustiere, Tiere, Säugetiere)
“stone” → Fern (lebloses Objekt)
“cuddle” → Näher an Tieren 
(Handlung, die mit Lebewesen assoziiert wird)
Multivektorraum
n Dimensionen (GPT 3.5 ~ 15.000)
Positionen entsteht aus dem Pre Training



Einbettungen wandeln diskrete Token (Wörter) in kontinuierliche numerische Vektoren in einem hochdimensionalen Raum um.


Embeddings
3Blue1Brown. But what is a GPT? Visual intro to transformers | Chapter 5, Deep Learning
13

3Blue1Brown. But what is a GPT? Visual intro to transformers | Chapter 5, Deep Learning
The King doth wake tonight and takes his rouse
“Modern Englisch”
The King wakes up tonight and begins his celebration


 
The King doth wake tonight and takes his rouse
“Shakespearean English”

The King wakes up tonight and begins his celebration, cat dog stone hybrid


The King wakes up tonight and begins his celebration, cat dog stone hybrid
The King wakes up tonight and begins his celebration, cat stone
The King wakes up tonight and begins his celebration, stoned cat
The King wakes up tonight and begins his celebration, cat dog stone hybrid
The King wakes up tonight and begins his celebration, cat 
cat 
dog 
stone 
hybrid
The King wakes up tonight and begins his celebration, dog hybrid



“On the Biology of a Large Language Model”
Anthropic. Interpretability: Understanding how AI models think. https://youtu.be/fGKNUvivvnc 
Anthropic. On the Biology of a Large Language Model. https://transformer-circuits.pub/2025/attribution-graphs/biology.html
Anthropische Forscher haben die internen Berechnungen von Claude 3.5 Haiku nachverfolgt. Sie fanden echte mehrstufige „Argumentationen” (Dallas → Texas → Austin), vorausschauende Planung in der Poesie (Aktivierung von Reimwörtern vor dem Schreiben von Zeilen) und sprachunabhängige Darstellungen in Englisch, Französisch und Chinesisch.

Das Modell führt diese Berechnungen automatisch durch, ohne metakognitives Bewusstsein für seine eigenen Prozesse.

LLMs as Retrieval-ish Systems

“LLMs are stores of knowledge and programs - they've stored pattern from the internet as vector programs” (François Chollet)

“Large language models is for me a database technology. It's not artificial intelligence.”(Sepp Hochreiter)


“LLMs are n-gram models on steroids doing approximate retrieval, not reasoning” (Subbarao Kambhampati)
LSTM: The Comeback Story?. https://youtu.be/8u2pW2zZLCs 
Prof. Sepp Hochreiter: A Pioneer in Deep Learning. https://youtu.be/IwdwCmv_TNY 
Prof. Dr. Sepp Hochreiter: KI Entwicklung, LSTM, OpenAI | Eduard Heindl Energiegespräch #100
https://youtu.be/LG1If4ccEDc 
Pattern Recognition vs True Intelligence - Francois Chollet. https://youtu.be/JTU8Ha4Jyfc 
François Chollet on OpenAI o-models and ARC. https://youtu.be/w9WE1aOPjHc


(How) Do LLMs Reason? (Talk given at MILA/ChandarLab). https://youtu.be/VfCoUl1g2PI
(How) Do LLMs Reason/Plan?. https://youtu.be/VfCoUl1g2PI
AI for Scientific Discovery [Briefing & Panel Remarks at National Academies workshop on ]. https://youtu.be/TOIKa_gKycE 
DO REASONING MODELS ACTUALLY SEARCH?. https://youtu.be/2xFTNXK6AzQ 
LLMs rufen memorierte “Programme” aus dem latenten Raum ab und interpolieren zwischen ihnen, können aber nicht von gelernten Mustern abweichen. Ihre lückenhafte Generalisierung versagt bei Unbekanntem. Prompt Engineering sucht die optimalen Koordinaten für diese Programme.
LLMs erfassen menschliches Wissen aus Text/Code und speichern es. Aktuelles “Reasoning” wiederholt nur bereits gesehene Denkmuster. Sie können keine genuinen neuen Konzepte oder Denkansätze erschaffen. 
xLSTM wird als Alternative entwickelt.
Approximatives Retrieval täuscht Reasoning durch Mustererkennung vor, versagt aber bei Verschleierung und benötigt externe Verifizierer.

Prompt Engineering: “How to Prompt a Research Blog”

Properitäre vs. Open Source vs. Open Weights LLM
Claude Opus 4.1 | Claude Sonnet 4.5https://claude.ai 
Gemini 2.5 Prohttp://gemini.google.com | https://aistudio.google.com
GPT-5https://chatgpt.com 
Qwen3-Maxhttps://qwen.ai 
Mistralhttps://chat.mistral.ai 
Apertushttps://www.swiss-ai.org/apertus 
Aleph Alphahttps://aleph-alpha.com
…

ChatGPT ≠ AI ≠ GPT ≠ Machine Learning ≠ LLM ≠ Open Source ≠ Open Weights ≠ …

Wo LLM Probleme haben
Rechnen
Buchstabieren, Zählen
Websuche
Aktuelle Informationen
Räumliches und Zeitliches “Denken”
“Denken”
Stil (Sycophancy)
Nicht deterministisch, d. h. es kommen immer andere Antworten. 
Es gibt keinen Mechanismus in einem LLM, um etwas als “wahr” zu verifizieren → “Expert-in-the-Loop”

Assignment 1: Forschungsblog auf GitHub Pages
Deadline: 17.11.2025, 23:59 Uhr
Aufgabenstellung: Erweitere dein Projektkonzept aus Assignment 0 (Pitch) zu einem digitalen Forschungsblog und publiziere diesen über GitHub Pages. Der Blog soll deine Projektidee vertiefen und einen spezifischen Aspekt deines Vorhabens detailliert beleuchten. Nutze LLMs aktiv für die Texterstellung und die Entwicklung der Webseite – dokumentiere in deinem Prompt Engineering Journal, welche Prompts und Tools du verwendet hast und reflektiere den Einsatz!
Technische Anforderungen:
Static HTML Website (mindestens index.html)
CSS Stylesheet (extern oder embedded)
Publikation via GitHub Pages
Projektkürzel (=Repo Name)
Mindestens 3 Commits mit sinnvollen Commit-Messages
Responsive Design (mobile-friendly)


Assignment 1: Forschungsblog auf GitHub Pages
Inhaltliche Anforderungen:
Qualität > Länge
Beispiel: https://dhcraft.org/excellence/blog/System1-42 
Projektübersicht (erweitert aus 1-Pager)
Forschungsfrage und Kontext
Datenbeschreibung mit Beispielen
Methodischer Ansatz
Vertiefender Fokus (wählen EINEN Aspekt):
Datenaufbereitung: Herausforderungen und Lösungsansätze
Prompt Engineering: Geplante LLM-Interaktionen
Visualisierungskonzept: Darstellung der Ergebnisse
Forschungsrelevanz: Einbettung in Fachdiskurs
Mini-Projektplan
Meilensteine
Technologie-Stack
Erwartete Herausforderungen

GitHub & GitHub Pages
Was ist GitHub?

Versionskontrollplattform für Code und Texte
Basiert auf Git (Versionskontrollsystem)
Speichert alle Änderungen deines Projekts (Timeline)
Kollaboration: Mehrere Personen können am gleichen Projekt arbeiten
Repository = Projektordner in der Cloud

Was ist GitHub Pages?

Kostenloser Webhosting-Service von GitHub
Verwandelt dein Repository in eine Website
URL-Format: username.github.io/repository-name
Perfekt für statische Websites (HTML, CSS, JavaScript)
Automatische Publikation bei jedem Push


Setup in 4 Schritten
GitHub Account erstellen
Registrierung auf github.com 
GitHub Desktop installieren: https://desktop.github.com/download
Grafische Oberfläche für Git
Einfaches Commit & Push ohne Terminal
Repository erstellen & klonen
"New Repository" auf GitHub
Mit GitHub Desktop klonen
Lokal arbeiten, Änderungen committen & pushen
GitHub Pages aktivieren
Repository → Settings → Pages
Source: "Deploy from a branch"
Branch: main → / (root)
Nach 2-3 Minuten online unter username.github.io/repository-name

GitHub Pages

Hands-On: 1. Version auf GitHub Pages bringen
Verwende idealerweise ein Frontier-LLM und das kommt alles in deinen Ordner fürs Repo:
Akademisches Narrativ für einen Forschungsblog extrahieren und bearbeiten
Kümmere dich nicht so sehr um Formulierungen. Achte vielmehr auf das Narrativ.
Baue 1-2 Quellen ein (Paper, Daten, andere Ressorucen)
Lege ein ganz kleines DESIGN.md an (Design)
Lege ein ganz kleines RESEARCH-BLOG.md an (Inhalt)
Erzeuge eine einfache erste Webseite
index.html
style.css
Oder ein Python Script dass aus dem RESEARCH-BLOG.md den text lädt und ein HTML erzeugt.
Dokumentiere in einem Prompt Engineering Journal und mach auch sehr saubere Commits.

Hands-on: Extraktion des akademischen Narrativs
Manfred Thaller: Can historical information be displayed outside of a graph/hypergraph/network?:
```{copy/paste}
```
Analyse the text in detail and step by step and extract the academic narrative as list. 
Trennzeichen und Struktur

Kontextinformation

Arbeitsstil

Instruktionen

Art der Antwort definieren

Laden Sie keine PDF-/Word-Dateien direkt hoch, sondern kopieren Sie den Text und fügen Sie ihn ein (meistens besser)

Die Reihenfolge der Anweisungen ist (ein bisschen) wichtig.

Manfred Thaller: Can historical information be displayed outside of a graph/hypergraph/network?https://graphentechnologien.hypotheses.org/files/2021/02/Thaller-Mainz2021-2.pdf   

Follow-Up Prompts


Reread the text! What is not included? List and explain!
Write a detailed and concise report with ALL findings.
Is it perfect? List and explain. Be critical and honest!
Write the perfect report with ALL information!
Can the same information be presented more concisely without losing any of its complexity or detail?
Depending on the model used, you may need to rephrase these prompts to “find the better ‘programmes’”.

Think step by step how to represent that information in a diagram? use mermaid
Think step by step about how to present the academic narrative. How do you create the perfect slide deck?
Return the slides using Latex Beamer.
Create a Python script to create the PowerPoint.
https://www.mermaidchart.com 
https://de.overleaf.com
Think step by step about how to create a slide to give an overview and teach the content.
